# SuperstorePipeline
In this project, we will build a Data Pipeline that will:
1. Take superstore dataset from data world and split it to different datafiles and apply data cleanaing.
2. Build a data model and load the data to postgresql database using python.
3. Transfer the data from postgres to google cloud storage using python.
4. Build a dimensional model and load the data to snowflake data warehouse using airbyte.

**Architecture Diagram**


[
![DE](https://user-images.githubusercontent.com/39027056/196196234-8b6b9ff4-575c-4428-abc2-5dbfc6fcaef9.png)
](url)
